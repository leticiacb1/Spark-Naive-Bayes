{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b48e863-8e93-45c0-938d-c9a4566b69b8",
   "metadata": {},
   "source": [
    "# Projeto Spark\n",
    "\n",
    "Entrega: 31 de maio de 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4813f033-927b-46ca-8977-30e8b0de48c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introdução\n",
    "\n",
    "Neste projeto vamos construir um classificador Naive-Bayes para determinar o sentimento de um comentário."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f3a371",
   "metadata": {},
   "source": [
    "## Grupos\n",
    "\n",
    "O projeto pode ser individual ou em duplas. Criem os grupos em https://classroom.github.com/a/EdbpYF9x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7c97a3",
   "metadata": {},
   "source": [
    "## Instalando o ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b3746",
   "metadata": {},
   "source": [
    "O jeito mais simples de começar a trabalhar com Spark é instalar um container com tudo pronto! No site https://hub.docker.com/r/jupyter/pyspark-notebook vemos uma imagem Docker que já vem com `pyspark` e `jupyter lab`. Instale a imagem com o comando:\n",
    "\n",
    "```bash\n",
    "docker pull jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "Vamos iniciar o ambiente de trabalho com o comando `docker run`. Para isso precisamos tomar alguns cuidados:\n",
    "\n",
    "1) Temos que mapear nosso diretorio local de trabalho para um diretório interno do container, de modo que alterações feitas dentro do container (nesta pasta escolhida) sejam gravadas no nosso diretorio local. No container temos um usuário padrão com *username* `jovyan`. No *homedir* desse usuario temos uma pasta vazia `work`, que vai servir como local de mapeamento do nosso diretorio local de trabalho. Podemos então fazer esse mapeamendo com a opção `-v` do comando `docker run` da seguinte forma:\n",
    "\n",
    "```bash\n",
    "-v <diretorio>:/home/jovyan/work\n",
    "```\n",
    "\n",
    "onde `<diretorio>` representa seu diretorio local de trabalho.\n",
    "\n",
    "2) Para acessar o `jupyter notebook` e o *dashboard* do Spark a partir do nosso *browser* favorito temos que abrir algumas portas do container com a opção `-p`. As portas são `8888` (para o próprio `jupyter notebook`) e `4040` (para o *dashboard* do Spark). Ou seja, adicionaremos às opções do `docker run`o seguinte:\n",
    "\n",
    "```bash\n",
    "-p 8888:8888 -p 4040:4040\n",
    "```\n",
    "\n",
    "Desta forma, ao acessar `localhost:8888` na nossa máquina, estaremos acessando o servidor Jupyter na porta 8888 interna do container.\n",
    "\n",
    "3) Vamos iniciar o container no modo interativo, e vamos especificar que o container deve ser encerrado ao fechar o servidor Jupyter. Faremos isso com as opções `-it` e `-rm`\n",
    "\n",
    "Portanto, o comando completo que eu uso na minha máquina Linux para iniciar o container é:\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "    -it \\\n",
    "    --rm \\\n",
    "    -p 8888:8888 \\\n",
    "    -p 4040:4040 \\\n",
    "    -v \"`pwd`\":/home/jovyan/work \\\n",
    "    jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "Para facilitar a vida eu coloco esse comando em um arquivo `inicia.sh`. Engenheiros, façam do jeito que preferirem!\n",
    "\n",
    "Agora abra esse notebook lá no container!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090720d-0dec-48ff-a8da-921b62764c43",
   "metadata": {},
   "source": [
    "## Iniciando o Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603fa191-ca53-4035-9147-86f3cf27831a",
   "metadata": {},
   "source": [
    "Vamos iniciar o ambiente Spark. Para isso vamos:\n",
    "\n",
    "1) Criar um objeto de configuração do ambiente Spark. Nossa configuração será simples: vamos especificar que o nome da nossa aplicação Spark é \"Minha aplicação\", e que o *master node* é a máquina local, usando todos os *cores* disponíveis. Aplicações reais de Spark são configuradas de modo ligeiramente diferente: ao especificar o *master node* passamos uma URL real, com o endereço do nó gerente do *cluster* Spark.\n",
    "\n",
    "2) Vamos criar um objeto do tipo `SparkContext` com essa configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6c4a7e90-27ef-4593-a041-62848e553764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d1b7f8-ddc9-43b0-adbd-2b6e5aad182c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setAppName('Minha aplicação')\n",
    "conf.setMaster('local[*]')\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfcd1c3-3c9b-47c6-bf36-06c60b796991",
   "metadata": {},
   "source": [
    "O `SparkContext` é a nossa porta de entrada para o cluster Spark, ele será a raiz de todas as nossas operações com o Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5214f8ff-9df1-4817-9e08-c1755470f3b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://796d4f79f83a:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Minha aplicação</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Minha aplicação>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acb677-070b-4191-bbad-ee16000ff380",
   "metadata": {},
   "source": [
    "O link acima provavelmente não funcionará porque ele se refere à porta 4040 interna do container (portanto a URL está com endereço interno). Porém fizemos o mapeamento da porta 4040 interna para a porta 4040 externa, logo você pode acessar o *dashboard* do Spark no endereço http://localhost:4040\n",
    "\n",
    "<center><img src=\"./spark_dashboard.png\" width=800/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44bd07e-7f64-4c0d-b522-faf5313b53cc",
   "metadata": {},
   "source": [
    "## Lendo os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c2f3af",
   "metadata": {},
   "source": [
    "Vamos começar lendo o arquivo de reviews e gravando o resultado em formato pickle, mais amigável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7fda56-bf06-4309-a265-26e553e20b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    parts = line[1:-1].split('\",\"')\n",
    "    sentiment = int(parts[0])\n",
    "    title = parts[1].replace('\"\"', '\"')\n",
    "    body = parts[2].replace('\"\"', '\"')\n",
    "    return (sentiment, title, body)\n",
    "\n",
    "rdd = sc.textFile('train.csv').map(parse_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "900ea746-984c-405b-a449-d6cf8885525e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83f38e5-3d7b-4261-8ab3-caadb39d6076",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  'Stuning even for the non-gamer',\n",
       "  'This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339c7933-ad3f-4a17-bf89-8e1360ddc4ab",
   "metadata": {},
   "source": [
    "Agora vamos gravar no formato pickle, para facilitar os trabalhos futuros. Após gravar o arquivo, não mais rode as células desta primeira etapa!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39938d62-bc17-4243-a9e8-3fed9c8d2bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o53.saveAsObjectFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/jovyan/work/reviews.pickle already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.$anonfun$saveAsSequenceFile$1(SequenceFileRDDFunctions.scala:66)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.saveAsSequenceFile(SequenceFileRDDFunctions.scala:51)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsObjectFile$1(RDD.scala:1602)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.saveAsObjectFile(RDD.scala:1602)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:579)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile$(JavaRDDLike.scala:578)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsPickleFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreviews.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:3323\u001b[0m, in \u001b[0;36mRDD.saveAsPickleFile\u001b[0;34m(self, path, batchSize)\u001b[0m\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3322\u001b[0m     ser \u001b[38;5;241m=\u001b[39m BatchedSerializer(CPickleSerializer(), batchSize)\n\u001b[0;32m-> 3323\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mser\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsObjectFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o53.saveAsObjectFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/jovyan/work/reviews.pickle already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.$anonfun$saveAsSequenceFile$1(SequenceFileRDDFunctions.scala:66)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.SequenceFileRDDFunctions.saveAsSequenceFile(SequenceFileRDDFunctions.scala:51)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsObjectFile$1(RDD.scala:1602)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.saveAsObjectFile(RDD.scala:1602)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:579)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsObjectFile$(JavaRDDLike.scala:578)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsObjectFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "rdd.saveAsPickleFile('reviews.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f5b5b-f0ac-4385-866b-8d8d8ecbdf9f",
   "metadata": {},
   "source": [
    "## Um classificador Naive-Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ef2e6-ac48-46ad-b538-920c17aff6dc",
   "metadata": {},
   "source": [
    "Vamos ler o arquivo pickle gravado anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9107ce80-0447-41d7-8f83-d96f0680d8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd = sc.pickleFile('reviews.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae84b476-b262-496f-93b0-19386fa9f29c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "607f7afa-43e7-4b82-bffc-7c1cca0536ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  'Does what they say',\n",
       "  \"This lift does a great job. I had it installed in about one hour by myself. But we have a loft-type ceiling. My wife loved it so much we are buying two or three more. If I get her help this time, it probably won't take as long.\")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bfd91b4-2334-402e-a97d-1011610a7195",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rdd.take(1)\n",
    "len(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9751aa-df6d-4e3e-a8c5-039133cefe28",
   "metadata": {},
   "source": [
    "Agora, complete as tarefas em sequencia para construir o classificador Naive-Bayes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bac409-7181-49c4-b555-73d3e9ff71f2",
   "metadata": {},
   "source": [
    "### Fase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef78654e-d064-41b4-9be8-306a57c425f4",
   "metadata": {},
   "source": [
    "#### Tarefa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d82f8-520e-49de-8397-5277d5bd072e",
   "metadata": {},
   "source": [
    "Construa uma função que recebe um RDD no formato do RDD original e retorna um RDD no qual cada item é um par (palavra, contagem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb48e97-e57d-4b6a-9b27-b52cd9c3d195",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4', 115578)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remover_pontuacao(texto):\n",
    "    padrao = r\"[^\\w\\s]\"    \n",
    "    texto_sem_pontuacao = re.sub(padrao, \"\", texto)\n",
    "    return texto_sem_pontuacao\n",
    "\n",
    "\n",
    "def contar_palavras(rdd):\n",
    "    rdd = rdd.flatMap(lambda x: (x[1] + ' ' + x[2]).strip().lower().split())\n",
    "    \n",
    "    rdd = rdd.map(remover_pontuacao)\n",
    "    \n",
    "    rdd = rdd.filter(lambda x: x != \"\")\n",
    "\n",
    "\n",
    "    pares = rdd.map(lambda palavra: (palavra, 1))\n",
    "    \n",
    "    contagens = pares.reduceByKey(lambda x, y: x + y)\n",
    "    \n",
    "    return contagens\n",
    "\n",
    "contagens = contar_palavras(rdd)\n",
    "contagens.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b349948-fa10-45ca-8232-55fef2de85ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4', 115578),\n",
       " ('im', 343336),\n",
       " ('bikes', 3142),\n",
       " ('came', 126367),\n",
       " ('especially', 90730),\n",
       " ('like', 1016054),\n",
       " ('out', 807325),\n",
       " ('clearance', 1522),\n",
       " ('start', 83620),\n",
       " ('item', 124508),\n",
       " ('installation', 10916),\n",
       " ('holds', 21137),\n",
       " ('common', 23424),\n",
       " ('complain', 8697),\n",
       " ('family', 91251),\n",
       " ('attaching', 1323),\n",
       " ('book', 1926132),\n",
       " ('willing', 11392),\n",
       " ('manager', 4001),\n",
       " ('references', 11961)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contagens.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40c3d8d5-8fdd-45e7-866e-a9417667c43d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  'Does what they say',\n",
       "  \"This lift does a great job. I had it installed in about one hour by myself. But we have a loft-type ceiling. My wife loved it so much we are buying two or three more. If I get her help this time, it probably won't take as long.\")]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d888c4-bef5-4d1c-a901-e8dc5029f1b0",
   "metadata": {},
   "source": [
    "#### Tarefa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec5621-6c06-4dce-b351-e887d6aa8be4",
   "metadata": {},
   "source": [
    "Construa uma função que recebe o RDD (palavra, contagem) construido anteriormente e retorna um RDD no qual cada item é um par (palavra, $\\log_{10}\\left(c \\, / \\, T\\right)$), onde $c$ é a contagem daquela palavra e $T$ é a soma das contagens de palavra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca379b49-a447-4ecf-9946-6c408e2adc97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4', -3.3864777882880177)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calcular_log_probabilidades(rdd):\n",
    "    T = contagens.map(lambda x: x[1]).reduce(lambda x, y: x + y)\n",
    "    \n",
    "    rdd = rdd.map(lambda x: (x[0], math.log(x[1]/T, 10)))\n",
    "    \n",
    "    return rdd\n",
    "                  \n",
    "\n",
    "contagensLog = calcular_log_probabilidades(contagens)\n",
    "contagensLog.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09116064-380c-43ed-91a3-736c80b47fb9",
   "metadata": {},
   "source": [
    "#### Tarefa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df9afe-f429-4951-954d-8ac0361efee6",
   "metadata": {},
   "source": [
    "Separe o RDD original em dois RDDs: o dos reviews positivos e o dos negativos. Em seguida, use as funções anteriores para construir RDDs que contem os pares (palavra, $\\log_{10}\\left(c \\, / \\, T\\right)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "987554e0-0a16-4827-9cc3-7c21063b1f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd_positivo = rdd.filter(lambda x: x[0] == 2)\n",
    "rdd_negativo = rdd.filter(lambda x: x[0] == 1)\n",
    "\n",
    "rdd_positivo = contar_palavras(rdd_positivo)\n",
    "rdd_negativo = contar_palavras(rdd_negativo)\n",
    "\n",
    "rdd_positivo = calcular_log_probabilidades(rdd_positivo)\n",
    "rdd_negativo = calcular_log_probabilidades(rdd_negativo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfe5957-73d2-45ad-9ad2-6e029668bacf",
   "metadata": {},
   "source": [
    "### Tarefa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cb0c8-2236-47aa-8ab0-3ad925bb24ed",
   "metadata": {},
   "source": [
    "Use o `.fullOuterJoin()` dos RDDs para construir um RDD unificado, no qual cada item é da forma (palavra, log_prob_positivo, log_prob_negativo). \"Baixe\" esse resultado final usando `.collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57d6498a-8e0a-4541-9402-2cbf9a050541",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bikes', -5.213319816190182),\n",
       " ('came', -3.666658232102368),\n",
       " ('4', -3.6793704103783034),\n",
       " ('out', -2.8930396053141987),\n",
       " ('clearance', -5.680714862060204),\n",
       " ('start', -3.8015007392502964),\n",
       " ('like', -2.7696507664476715),\n",
       " ('item', -3.913110256469499),\n",
       " ('installation', -4.7372928208467435),\n",
       " ('holds', -4.3019239946091625)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_positivo.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2279222-1b6e-41d3-8b0b-a0767a70b0aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4', -3.695800543554358),\n",
       " ('im', -3.167463220950031),\n",
       " ('especially', -3.936175330828775),\n",
       " ('like', -2.7187709290231235),\n",
       " ('out', -2.7987385306226753),\n",
       " ('clearance', -5.4785413524353),\n",
       " ('came', -3.631549689342251),\n",
       " ('book', -2.48269742386178),\n",
       " ('willing', -4.632714018409357),\n",
       " ('manager', -5.112093423557543)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_negativo.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b599156-e71d-46b4-9499-8d2dfe072512",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rdd_joined \u001b[38;5;241m=\u001b[39m \u001b[43mrdd_positivo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfullOuterJoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd_negativo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m rdd_joined\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:3787\u001b[0m, in \u001b[0;36mRDD.fullOuterJoin\u001b[0;34m(self, other, numPartitions)\u001b[0m\n\u001b[1;32m   3741\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfullOuterJoin\u001b[39m(\n\u001b[1;32m   3742\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple[K, V]]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3743\u001b[0m     other: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple[K, U]]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3744\u001b[0m     numPartitions: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3745\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple[K, Tuple[Optional[V], Optional[U]]]]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3746\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3747\u001b[0m \u001b[38;5;124;03m    Perform a right outer join of `self` and `other`.\u001b[39;00m\n\u001b[1;32m   3748\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3785\u001b[0m \u001b[38;5;124;03m    [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\u001b[39;00m\n\u001b[1;32m   3786\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpython_full_outer_join\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumPartitions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/join.py:101\u001b[0m, in \u001b[0;36mpython_full_outer_join\u001b[0;34m(rdd, other, numPartitions)\u001b[0m\n\u001b[1;32m     98\u001b[0m         wbuf\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ((v, w) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vbuf \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m wbuf)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_do_python_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumPartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/join.py:41\u001b[0m, in \u001b[0;36m_do_python_join\u001b[0;34m(rdd, other, numPartitions, dispatch)\u001b[0m\n\u001b[1;32m     39\u001b[0m vs \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapValues(\u001b[38;5;28;01mlambda\u001b[39;00m v: (\u001b[38;5;241m1\u001b[39m, v))\n\u001b[1;32m     40\u001b[0m ws \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mmapValues(\u001b[38;5;28;01mlambda\u001b[39;00m v: (\u001b[38;5;241m2\u001b[39m, v))\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mws\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgroupByKey(numPartitions)\u001b[38;5;241m.\u001b[39mflatMapValues(\u001b[38;5;28;01mlambda\u001b[39;00m x: dispatch(x\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1246\u001b[0m, in \u001b[0;36mRDD.union\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;124;03mReturn the union of this RDD and another one.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;124;03m[1, 1, 2, 3, 1, 1, 2, 3]\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer \u001b[38;5;241m==\u001b[39m other\u001b[38;5;241m.\u001b[39m_jrdd_deserializer:\n\u001b[1;32m   1245\u001b[0m     rdd: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Union[T, U]]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m RDD(\n\u001b[0;32m-> 1246\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241m.\u001b[39munion(other\u001b[38;5;241m.\u001b[39m_jrdd), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer\n\u001b[1;32m   1247\u001b[0m     )\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;66;03m# These RDDs contain data in different serialized formats, so we\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;66;03m# must normalize them to the default serializer.\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     self_copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reserialize()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:5441\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5439\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 5441\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5442\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[1;32m   5443\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[1;32m   5447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[1;32m   5448\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:5241\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   5239\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5240\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 5241\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5242\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSimplePythonFunction(\n\u001b[1;32m   5244\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m   5245\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5250\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[1;32m   5251\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:5226\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   5224\u001b[0m pickled_command \u001b[38;5;241m=\u001b[39m ser\u001b[38;5;241m.\u001b[39mdumps(command)\n\u001b[1;32m   5225\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 5226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   5227\u001b[0m     \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n\u001b[1;32m   5228\u001b[0m     broadcast \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mbroadcast(pickled_command)\n\u001b[1;32m   5229\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m ser\u001b[38;5;241m.\u001b[39mdumps(broadcast)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "rdd_joined = rdd_positivo.fullOuterJoin(rdd_negativo)\n",
    "rdd_joined.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e90473-6725-4f30-9130-f5a0f370d968",
   "metadata": {},
   "source": [
    "#### Tarefa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d2117-397c-4a66-b63b-ca00036c1e2d",
   "metadata": {},
   "source": [
    "Para uma dada string, determine se ela é um review positivo ou negativo usando os RDDs acima. Lembre-se de como funciona o classificador Naive-Bayes: http://stanford.edu/~jurafsky/slp3/slides/7_NB.pdf, consulte tambem suas notas de aula de Ciência dos Dados!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2f68949-61c8-46b4-a39e-bcfb3872c4fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 44\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Adicionando \"coluna\" para verificar teste do classificador\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# classification=[]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# rdd_result = rdd\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# reviews.take(5)\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m column_values \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Converta o RDD em uma lista usando o método collect()\u001b[39;00m\n\u001b[1;32m     47\u001b[0m column_list \u001b[38;5;241m=\u001b[39m column_values\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:745\u001b[0m, in \u001b[0;36mRDD.map\u001b[0;34m(self, f, preservesPartitioning)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(_: \u001b[38;5;28mint\u001b[39m, iterator: Iterable[T]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[U]:\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(fail_on_stopiteration(f), iterator)\n\u001b[0;32m--> 745\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitionsWithIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreservesPartitioning\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:871\u001b[0m, in \u001b[0;36mRDD.mapPartitionsWithIndex\u001b[0;34m(self, f, preservesPartitioning)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmapPartitionsWithIndex\u001b[39m(\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    834\u001b[0m     f: Callable[[\u001b[38;5;28mint\u001b[39m, Iterable[T]], Iterable[U]],\n\u001b[1;32m    835\u001b[0m     preservesPartitioning: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    836\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[U]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    837\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;124;03m    Return a new RDD by applying a function to each partition of this RDD,\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;124;03m    while tracking the index of the original partition.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;124;03m    6\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPipelinedRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreservesPartitioning\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:5421\u001b[0m, in \u001b[0;36mPipelinedRDD.__init__\u001b[0;34m(self, prev, func, preservesPartitioning, isFromBarrier)\u001b[0m\n\u001b[1;32m   5419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bypass_serializer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   5420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitioner \u001b[38;5;241m=\u001b[39m prev\u001b[38;5;241m.\u001b[39mpartitioner \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 5421\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier \u001b[38;5;241m=\u001b[39m isFromBarrier \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mprev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_barrier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:5123\u001b[0m, in \u001b[0;36mRDD._is_barrier\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_barrier\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   5120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5121\u001b[0m \u001b[38;5;124;03m    Whether this RDD is in a barrier stage.\u001b[39;00m\n\u001b[1;32m   5122\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39misBarrier()\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "def NaiveBayes(review, wordsProb):    \n",
    "    log_prob_positivo_total = 0\n",
    "    log_prob_negativo_total = 0\n",
    "    \n",
    "    for word in review.split():\n",
    "        if word in wordsProb.map(lambda x: x[0]):\n",
    "            log_prob_positivo, log_prob_negativo = wordsProb.filter(lambda x: x[0] == word).first()[1]\n",
    "            if log_prob_positivo is not None:\n",
    "                log_prob_positivo_total += log_prob_positivo\n",
    "            if log_prob_negativo is not None:\n",
    "                log_prob_negativo_total += log_prob_negativo\n",
    "    \n",
    "    if log_prob_positivo_total > log_prob_negativo_total:\n",
    "        return 2\n",
    "    return 1\n",
    "\n",
    "\n",
    "# Adicionando \"coluna\" para verificar teste do classificador\n",
    "# classification=[]\n",
    "# rdd_result = rdd\n",
    "# reviews = rdd_result.map(lambda x: x[2]).collect()\n",
    "# for review in reviews:\n",
    "#     result = NaiveBayes(review, rdd_joined)\n",
    "    \n",
    "#     value_real = rdd_result.filter(lambda x: x[2] == review).first()[0]\n",
    "    \n",
    "#     classification.append(value_real, result)\n",
    "    \n",
    "# word = 'adney'\n",
    "# try:\n",
    "#     print(rdd_joined.filter(lambda x: x[0] == word).first())\n",
    "#     x,y = rdd_joined.filter(lambda x: x[0] == word).first()[1]\n",
    "#     print(x,y)\n",
    "# except:\n",
    "#     print(\"não tem\")\n",
    "    \n",
    "# v = rdd_joined.map(lambda x: (x, ''))\n",
    "# rdd_result.take(10)\n",
    "\n",
    "# print(classification[0:10])\n",
    "\n",
    "# reviews.take(5)\n",
    "\n",
    "column_values = rdd.map(lambda x: x[2])\n",
    "\n",
    "# Converta o RDD em uma lista usando o método collect()\n",
    "column_list = column_values.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab052fff-3752-4476-88f5-bc6b654c8e02",
   "metadata": {},
   "source": [
    "### Fase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071fa13-c31b-434e-969f-538c0070fb34",
   "metadata": {},
   "source": [
    "Agora que temos um classificador Naive-Bayes, vamos explorá-lo um pouco:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed5be5-9e8d-4a7f-98bc-592cc7ecee74",
   "metadata": {},
   "source": [
    "### Tarefa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9bd58-1023-4cb7-861e-6cdbe5384356",
   "metadata": {},
   "source": [
    "Quais são as 100 palavras que mais indicam negatividade, ou seja, onde a diferença entre a probabilidade da palavra no conjunto dos comentários negativos e positivos é máxima? E quais as 100 palavras de maior positividade? Mostre os resultados na forma de *word clouds*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147d964-8720-43fa-a8e3-0610394a752f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8854a0bf-1941-47eb-a1f5-6b3f4d34f857",
   "metadata": {},
   "source": [
    "### Tarefa desafio!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a98d11-16d6-4d97-b1ab-14db145e826e",
   "metadata": {},
   "source": [
    "Qual o desempenho do classificador (acurácia)? Para medir sua acurácia:\n",
    "\n",
    "- Separe os reviews em dois conjuntos: treinamente e teste\n",
    "- Repita o \"treinamento\" do classificador com o conjunto de treinamento\n",
    "- Para cada review do conjunto de teste, determine se é positiva ou negativa de acordo com o classificador\n",
    "- Determine a acurácia\n",
    "\n",
    "Esta não é uma tarefa trivial. Não basta fazer um `for` para determinar a classe de cada review de teste: isso demoraria uma eternidade. Você tem que usar variáveis \"broadcast\" do Spark para enviar uma cópia da tabela de frequencias para cada *core* do executor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a2c0d-7bfc-40bf-b717-1414c1df05ec",
   "metadata": {},
   "source": [
    "### Tarefa desafio!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af21c4e-5a5b-4127-80a0-d79e95f03b8f",
   "metadata": {},
   "source": [
    "Implemente Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61c989-ec58-4ebe-be19-595ad9e4887c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08805b7b-f8b5-4b85-ab53-3c0813f79c44",
   "metadata": {},
   "source": [
    "## Rubrica de avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74059567-1f3d-414a-bbc8-f5a7ea144b76",
   "metadata": {},
   "source": [
    "- I: groselha, falha crítica, ou não entregou nada\n",
    "- D: Fez uma tentativa honesta de fazer todos os itens da fase 1, mas tem erros\n",
    "- C: Fase 1 completa\n",
    "- B: Fase 2, faltando apenas um desafio\n",
    "- A: Fase 2 completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33106eb-7a38-4d89-8a23-c14fce370bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
